# Miner√≠a de textos - Tema 3 - Representaci√≥n de documentos.


Para que los algoritmos de miner√≠a de textos puedan procesar un texto se deben extraer sus caracter√≠sticas o rasgos. El contenido textual se puede representar de diferentes formas (cadenas de caracteres, palabras, estructuras sint√°cticas, grafos de relaciones, predicados, etc.) . a representaci√≥n de un texto deber√° ser fiel a su contenido, incluyendo la informaci√≥n necesaria para poder extraer el conocimiento √∫til que se espera obtener. Adem√°s, deber√° ser adecuada a las especificaciones de los algoritmos que se empleen a continuaci√≥n. Diferentes tareas de la miner√≠a de textos y diferentes tipos de textos pueden requerir diferentes representaciones. En [Hothoet al 2005] se presentan los principios de la miner√≠a de textos y se detallan los modelos de representaci√≥n de textos m√°s habituales

## Modelos de representaci√≥n

Los m√©todos de representaci√≥n m√°s utilizados son:

- Modelodel espacio vectorial(Vector Space Model ‚ÄìVSM-)
- Modelo probabil√≠stico(Probabilistic Topic Model)
- Modelo estad√≠stico(Statistical Language Model)
- Modelos sem√°nticos vectoriales (Vector SemanticModels)

Nos centraremos en el modelo del espacio vectorial y en el sem√°ntico vectorial pero se dar√°n nociones del resto de modelos

### Espacio vectorial

Un modelo de representaci√≥n vectorial de documentos se puede definir como una cu√°drupla $<X, B, \mu, F>$ donde:

- Vocabulario X: conjunto de cadenas posibles.
- √Ålgebra B: generaliza las relaciones entre los objetos
- Funci√≥n de medida o m√©trica $\mu$: establece la distancia (o similitud) entre objetos
- Funci√≥n de pesado F: establece la proyecci√≥n de cada objeto del espacio sobre la recta real

Los documentos se modelan como conjuntos de rasgos (cadenas de caracteres: tokens, palabras, n-gramas, ‚Ä¶) que pueden ser individualmente tratados y pesados. Los documentos pasan a ser representados como vectores dentro de un espacio eucl√≠deo, de forma que midiendo la distancia entre dos vectores se tratar√° de estimar su similitud como indicador de cercan√≠a sem√°ntica. Se pueden aplicar diferentes m√©tricas $\mu$ para el c√°lculo de la distancia: distancia eucl√≠dea y coseno son las m√°s populares pero hay muchas otras.

Asume el **"principio de independencia"** entre rasgos: las cadenas (rasgos) aparecidas en un mismo texto no tienen relaci√≥n entre s√≠ y se pueden cuantificar individualmente. No tiene en cuenta el orden en el que aparecen los rasgos en el texto: la sem√°ntica de un documento queda reducida a la suma de los significados de los rasgos que contiene. Aunque estas suposiciones no son correctas desde el punto de vista ling√º√≠stico, reducen la complejidad sin penalizar en muchos casos de forma sustancial los resultados.

Un documento quedar√° representado como una combinaci√≥n lineal de vectores base $t_i$ donde cada coeficiente $t_{ij}$ de la combinaci√≥n representar√° la relevancia de cada rasgo en el contenido del documento, calculada con una funci√≥n de pesado $F$.

As√≠, definir una representaci√≥n dentro del VSM se reduce aencontrar una funci√≥n de asignaci√≥n de pesos $F: f(t_i) = t_{ij}$ para calcular el valor de cada componente del vector $d_j$. Dos representaciones de un mismo documento $di(d_i y d'_i)$ a partir del mismo vocabulario ser√°n diferentes siempre que el conjunto de valores $t_{ij}$ que tomen las componentes de los vectores $d_i$ y $d'_i$ sean diferentes; es decir,

$$ d_j \neq d'_j \Leftrightarrow \{i| t_{ij} \neq t'_{ij}, \forall i \} \neq \emptyset $$

**Bolsa de palabras** (*Bag of Words, BoW*) es uno de los m√©todos m√°s b√°sicos de representarun documento. En la representaci√≥n BoW se representa un documento utilizando un vector con tantas componentes como palabras/t√©rminos haya en el vocabulario y el valor de la componente se calcula utilizando una funci√≥n de pesado.

#### Colecciones de documentos

Una colecci√≥n de documentos se suele representar mediante una **matriz t√©rmino-documento**, enla que cada fila representa una palabra en el vocabulario de la colecci√≥n y cada columna representa un documento de la colecci√≥n.

Este tipode matrices se utilizaron primeramente para determinar la similitud entre documentos en Recuperaci√≥n de Informaci√≥n(Information Retrieval): dos documentos similares tendr√°n palabras similares y eso quedar√° patente en sus vectores que tambi√©n ser√°n similares.

Los tama√±os de los vocabularios en colecciones extensas pueden ser de cientos de miles y el n√∫mero de documentos tambi√©n puede llegar a ser muy alto, lo que da lugara matrices dispersas (con muchos ceros).

#### Limitaciones

- **Alta dimensionalidad de la representaci√≥n**: suele dar como resultado vectores muy largos y dispersos en colecciones extensas (contienen principalmente ceros) ya que muchas las palabras nunca ocurren en el contexto de otras. Las funciones de selecci√≥n y reducci√≥n de rasgos ayudan a reducir la dimensionalidad.
- **P√©rdida de correlaci√≥n con las palabras (t√©rminos, rasgos) adyacentes**: algunas propuestas utilizan para representar un documento secuencias de s√≠mbolos (car√°cteres, palabras) denominadas n-gramas, donde n indica el tama√±o de la secuencia, o utilizan t√©rminos multipalabra que previamente deben ser identificados.
- **P√©rdida de posibles relaciones sem√°nticas entre los t√©rminos de un documento**: algunas propuestas usan el √çndice de Latencia Sem√°ntica (LSI) que es un m√©todo de an√°lisis de coaparici√≥n entre rasgos y se basa en el an√°lisis de latencia sem√°ntica (LatentSemanticAnalysis, LSA), otras utilizan representaciones basadas en ontolog√≠as [Hothoet al 2001].

### Modelo probabil√≠stico de temas (Probabilistic Topic Model)

Los documentos se representan como una mezcla de temas, donde un **tema** es una distribuci√≥n de probabilidad sobre palabras. Se asume que si un documento trata de un tema en particular, es de esperar que aparezcan palabras concretas en el documento con mayor o menor frecuencia: "carrocer√≠a", "llantas" o "volante" aparecer√°n m√°s a menudo en documentos sobre coches que en documentos de otros temas, mientras que palabras de uso general como "el", "de" o "es" aparecer√°n igualmente en todos.

Un documento t√≠picamente trata m√∫ltiples temas en diferentes proporciones; por lo tanto, en un documento que es 80% sobre coches probablemente habr√≠a sobre ocho veces m√°s palabras relacionadas con el tema ‚Äúcoches‚Äù que de otros temas.

Los "temas" producidos por las t√©cnicas de este modelado son grupos de palabras relacionadas. Un modelo tem√°tico examina un conjunto de documentos y descubre a partir de las estad√≠sticas de las palabras de cada uno cu√°les podr√≠an ser los temas y cu√°l es el equilibrio tem√°tico de cada documento. Este modelo supera los problemas asociados con "el t√©rmino‚Äù como tema. Un t√©rmino puede ser una palabra o una frase. Sin embargo este modelo no puede representar temas complejos, capturar las variaciones de vocabulario y la ambig√ºedad del sentido de las palabras


La Indexaci√≥n Sem√°ntica Latente Probabil√≠stica --Probabilistic Latent Semantic Indexing (PLSI)--y la Asignaci√≥n Latente de Dirichlet--Latent DirichletAllocation (LDA) --son los dos m√©todos de modelizaci√≥n de temas m√°s conocidos. M√°s informaci√≥n en https://en.wikipedia.org/wiki/Topic_model#Algorithms.

### Modelo de lenguaje estad√≠stico de temas(Statistical Language Model)

Es una distribuci√≥n de probabilidad sobre secuencias de palabras. Se aplica en muchas tareas relacionadas con procesamiento de lenguaje natural: reconocimiento de voz, traducci√≥n autom√°tica, clasificaci√≥n y enrutamiento de documentos, reconocimiento √≥ptico de caracteres, recuperaci√≥n de informaci√≥n, reconocimiento de escritura a mano, correcci√≥n ortogr√°fica ...

Uno de los modelos m√°s utilizado es el **modelo de n-gramas**. El modelode n-gramas asume que la probabilidad de una palabra depende de las n palabras anteriores. Es decir, la probabilidad $P(w_1,w_2,w_3,...,w_m)$ de observar la oraci√≥n $w_1$, $w_2$, ..., $w_m$ se aproxima de la siguiente manera:

$$ ùëÉ( ùë§_1,ùë§_2,‚Ä¶, ùë§_ùëö)=\prod_{i=1}^m p(ùë§_ùëñ|ùë§_1, ..., ùë§_{ùëñ‚àí1}) $$


Cuando en este modelo n=1, es decir no hay dependencia de palabras previas, es equivalente a la bolsa de palabras.

M√°s informaci√≥n en https://en.wikipedia.org/wiki/Language_modely en el cap√≠tulo 3 de [Jurafskyand Martin 2019]

### Modelo sem√°ntico vectorial

Se basan en la hip√≥tesis distribucional [Joos1950] que dice que las palabras que ocurren en contextos similares tienden a tener significados similares. Relacionan la similitud en la distribuci√≥n de las palabras y la similitud en su significado: palabras sin√≥nimas tienden a aparecer en contextos similares. Se hace corresponder la diferencia de significado entre dos palabras con la diferencia en los contextos en los que aparecen.

En este modelo a cada palabra se le asocia un vector y se representa como un punto en alg√∫n espacio sem√°ntico multidimensional.

En lugar de una matriz de t√©rmino-documento como en el modelo del espacio vectorial, se utiliza una **matriz palabra-palabra** (t√©rmino-t√©rmino) que tambi√©n se denomina matriz t√©rmino-contexto dado que las columnas son tambi√©n palabras, no documentos. La dimensionalidad de esta matriz para un vocabulario V ser√≠a $|V| \times |V|$ donde cada elemento o celda representar√≠a el n√∫mero de veces que la palabra de la fila (objetivo) y la palabra de la columna(contexto) coaparecen en un determinado contexto en un corpus.

Para determinar el tama√±o del contexto de la palabra objetivo normalmente se define una ventana de varias palabras por delante y por detras de una considerada objetivo y ese ser√≠a el contexto a tener en cuenta. En[Jurafskyand Martin 2019], Cap. 6, p√°g. 9‚Äî10 se puede ver un ejemplo de matriz t√©rmino-contexto de unos textos extra√≠dos de Wikipedia.

Para determinar la **similitud** entre dos palabras v y w se suele utilizar una medida de similitud entre sus correspondientes vectores: el coseno.

$$ sim(v,w) = cos(v,w) = \frac{v ¬∑ w}{|v||w|}$$

El valor del coseno var√≠a desde 1 para vectores que apuntan en la misma direcci√≥n, a trav√©s de 0 para vectores que son ortogonales, a -1 para vectores que apuntan en direcciones opuestas. Como los valores de frecuencia que contienen los vectores no son negativos el coseno var√≠a de 0 a 1.

#### Funci√≥n de pesado

Utilizar la frecuencia como pesado en los vectores t√©rmino-contexto no se considera la mejor opci√≥n para determinar asociaciones entre palabras ya que en el contexto de una palabra objetivo normalmente ocurren palabras frecuentes con todo tipo de palabras y no son informativas sobre ninguna palabra en particular(art√≠culos, determinantes, verbos auxiliares, ‚Ä¶). En lugar de la frecuencia se suele usar TF-IDF: en este modelo representa lo relevante que es una palabra con respecto a otra en una colecci√≥n y permite determinar que algunas palabras son generalmente m√°s comunes que otras en las colecciones.

Con este modelo se puede por ejemplo:
- encontrar las x palabras m√°s similar es a una objetivo calculando los cosenos entre dicha palabra y el resto de las palabras del vocaculario, ordenar el resultado y seleccionar las x con valores m√°s altos.
- decidir si dos documentos son similares considerando los vectores de todas las palabras de un documento y calculando el centroide de todos los vectores. Dados dos documentos podemos calcular sus vectores documento y estimar su similitud utilizando el coseno.

Una funci√≥n de ponderaci√≥n alternativa a TF-IDF es Informaci√≥n Mutua Puntual Positiva PPMI (Positive Pointwise Mutual Information).
PMI es una medida de la frecuencia con la que ocurren dos eventos x e y, comparados con lo que esperar√≠amos si fueran independientes. Como puede dar como resultado valores negativos, se utiliza PPMI, que en vez de valores negativos devuelve 0.

#### Embeddings

Si se representa una palabra como un vector con las dimensiones de todo el vocabulario muy probablemente el vector ser√° disperso (muchos elementos con valor 0) pero se puede limitar el numero de las dimensiones de forma que la mayor√≠a de los valores de los vectores sean distintos de cero, dando lugar a vectores densos. Dimensiones habituales son valores como 256, 512, o 1024.

La idea de la sem√°ntica vectorial es representar una palabra como un punto en alg√∫n espacio sem√°ntico multidimensional, las representaciones resultantes tambi√©n reciben el nombre de representaciones distribuidas. Los vectores para representar palabras se llaman embeddings(embebidos) porque la palabra est√° embebida en un espacio vectorial en particular. Los modelos sem√°nticos vectoriales son muy pr√°cticos porque se pueden aprender autom√°ticamente a partir de texto sin necesidad de etiquetado o supervisi√≥n. Actualmenteson la forma habitual de representaci√≥n en muchas tareas de procesamiento de lenguaje natural: traducci√≥n autom√°tica, an√°lisisde opinion, generaci√≥n de textos, Chatbox(sistemas que responden preguntas de usuarios), ...

Para trabajar con representaciones distribuidas una opci√≥n esgenerar nuestro propio modelo de vectoreso bien se puede partir de un modelo ya entrenado. Estas representaciones permiten utilizar vectores densos, en los que se limita el tama√±o de las dimensiones y la mayor√≠a de los valores de los vectores son distintos de cero. Hay variosmodelosde word embedding disponibles: word2vec (Google); Glove (Stanford, [Pennington et al. 2014] ); fastTest(Facebook, https://fasttext.cc/); y m√°srecientementeel modelode Bert (Google, https://arxiv.org/pdf/1810.04805.pdf) o Elmo (https://allennlp.org/elmo).

#### Word2vec

Herramienta para entrenar/generar y usar wordembeddings que utiliza modelos predictivos. Los modelos predictivos intentan predecir directamente una palabra a partir de sus vecinos en t√©rminos de vectores peque√±os y densos que se aprenden durante el entrenamiento a partir del corpus sin etiquetar. La idea detr√°s de estos m√©todos es que si podemos predecir en qu√© contexto aparece una palabra, entonces significa que entendemos el significado de la palabra en su contexto. Su objetivo es agrupar vectores de palabras similares en el espacio multidimensional. Word2vec utiliza una red neuronal de 2 capas que toma como entrada un corpus textual y genera sus correspondientes word embeddings que pueden ser la entrada de una red de aprendizaje profundo o servir para detectar relaciones entre palabras.

Word2Vec implementa dos modelos neuronales:
- CBOW: el modelo predice la palabra objetivo dada una ventana de palabras (el contexto)
- Skip-gram: el modelo predice las palabras contexto a partir de la palabra objetivo

#### FastText[Bojanowskiet al. 2017]

Es una extensi√≥n del modelo Word2Vec en el que cada palabra se trata como la suma de sus composiciones de n-gramas de caracteres. El vector para una palabra est√° compuesto por la suma de sus n-gramas de caracteres. Por ejemplo el vector para la palabra ‚Äúapple‚Äù estar√≠a compuesto por la suma los vectores para los n-gramas "<ap, app, appl, apple>, ppl, pple, pple>, ple, ple>, le> ‚Ä¶". El objetivo es obtener mejores representaciones para palabras poco frecuentes y poder generar vectores para palabras que no se encuentran en el vocabulario de los wordembeddings. 

#### Modelos contextualizados: Bert

Las representaciones preentrenadas pueden ser independientes de contexto o contextualizadas y estas √∫ltimas unidireccionales o bidireccionales. Los modelos independientes del contexto como word2vec o GloVe generan una √∫nica representaci√≥n vectorial para una palabra ("banco" de entidad financiera y "banco" de sentarse tienen la misma representaci√≥n). Los modelos contextualizados generan una representaci√≥n de cada palabra que est√° basada en las otras palabras de la oraci√≥n en la que aparece: unidireccional si solo tiene en cuenta las previas y bidireccional si tiene en cuenta las previas y las siguientes.

Bert es un modelo contextualizado para el ingl√©s. Existen modelos de lenguaje contextualizados para diferentes lenguas:
- Espa√±ol: RigoBERTa (https://www.iic.uam.es/inteligencia-artificial/procesamiento-del-lenguaje-natural/modelo-lenguaje-espanol-rigoberta/)
- Espa√±ol: BETO (https://github.com/dccuchile/beto)
- Multilingue: Bert multiling√ºe hasta 104 lenguas dependiendo de la versi√≥n (https://github.com/google-research/bert/blob/master/multilingual.md)

## Funciones de pesado

Las funciones de pesado (term weighting functions) constituyen funciones de proyecci√≥n F dentro de la definici√≥n formal del modelo vectorial de representaci√≥n de documentos. Se pueden identificar 2 tipos: locales y globales.

### Funciones locales.
Aquellas que toman √∫nicamente informaci√≥n del propio documento para obtener el peso de un rasgo en un documento.

#### Binaria

$$
F: Bin(t_i, t_j)= = \begin{cases}
    1 & \text{si el rasgo } t_i \text{ aparece en } d_j \\ 
    0 & \text{si no.}
\end{cases}
$$

#### Term Frequency

$$ F: TF(t_i, d_j) = f_{ij}$$

donde $f_{ij}$ es la frecuencia del rasgo en el documento.

#### Weighted Term Frequency

Proporci√≥n de la frecuencia respecto al total de t√©rminos del documento.

$$ F: WTF(t_i, d_j) = \frac{f_{ij}}{\sum_{t_p \in d_j} f_{pj}}$$

#### Augmented Normalized Term Frequency

Para compensar el sesgo que pueden introducir los documentos m√°s largos. El denominador
corresponde a la frecuencia m√°xima en el documento

$$ F: ANTF(t_i, d_j) = 0.5 + 0.5\frac{f_{ij}}{\max(f_{pj} | t_p \in d_j)}$$

#### Normalizaci√≥n Logar√≠tmica

$$ F: L(t_i, d_j) = \log_2(f_{ij} + 1)$$

### Funciones globales

Aquellas que toman informaci√≥n de la colecci√≥n para obtener el peso de un rasgo en un documento. Para su c√°lculo suele ser necesario el uso de un fichero invertido con informaci√≥n relativa a las frecuencias de los rasgos en cada documento de la colecci√≥n.

#### Frecuencia inversa del documento

Medida de si el t√©rmino es com√∫n o no en la colecci√≥n de documentos

$$ F: IDF(t_i, d_j) = \log \frac{N}{1 + df(t_i)} $$

donde $df(t_i)$ es el n¬∫ de documentos de la colecci√≥n en los que aparece el rasgo $t_i$ y N el n¬∫ documentos total, se suma 1 al denominador ya que podr√≠a de otra forma ser 0 si el t√©rmino no est√° en la colecci√≥n.

#### Frecuencia del T√©rmino x Frecuencia inversa de Documento (TF-IDF)

Un valor alto TF-IDF se alcanza con una elevada frecuencia del t√©rmino (en el documento dado) y una peque√±a frecuencia de ocurrencia del t√©rmino en la colecci√≥n completa de documentos. El cociente dentro de la funci√≥n logaritmo del idf es siempre mayor o igual que 1, el valor de IDF y de TF-IDF es mayor o igual que 0. Cuando un t√©rmino aparece en muchos documentos el valor de TF-IDF se acerca a 0.

Muy habitual en tareas de miner√≠a de textos. [Ver video](
https://www.coursera.org/lecture/ml-clustering-and-retrieval/document-representation-nl267)

$$ F: \operatorname{TF-IDF}(t_i, d_j) = TF \times IDF = f_{ij} \log \frac{N}{1 + df(t_i)}$$


#### Frecuencia inversa Probabil√≠stica

$$ F: PIF(t_i, d_j) = \log \frac{N - df(t_i)}{df(t_i)} $$

#### Funci√≥n Normal

$$ F: N(t_i, d_j) = \sqrt{\frac{1}{\sum_{d_k \in C} f_{ij}^2}} $$

#### Frecuencia Global x Frecuencia Inversa de Documento

$$ F: \operatorname{GF-IDF}(t_i, d_j) = \frac{\sum_{d_{j}\in C }f_{ij}}{df(t_i)}$$


## Selecci√≥n y reducci√≥n de rasgos

La **selecci√≥n de rasgos** consiste en la transformaci√≥n de un texto en el conjunto de rasgos que lo podr√°n representar. Normalmente estas transformaciones b√°sicas se encuadran en lo que se denomina preprocesamientode los textos.

Tareas b√°sicas de preprocesamientoson (ver tema 1):
- Tokenizaci√≥n
- Normalizaci√≥n l√©xica
- Truncado (Stemming)
- Lematizaci√≥n
- Eliminaci√≥n de palabras vac√≠as (stopwords)

La **reducci√≥n de rasgos** permite disminuir la dimensi√≥n de las representaciones que puede llegar a ser muy alta para colecciones de gran tama√±o. Permite realizar una ponderaci√≥n en base a la cual se pueden ordenar todos los rasgos de un vocabulario para seleccionar un subconjunto de ellos. La selecci√≥n se puede hacer estableciendo un umbral de ponderaci√≥n m√≠nima o bien prefijando una dimensi√≥n reducida, generando as√≠ un vocabulario que resultar√° ser un subconjunto del vocabulario inicial. En muchas ocasiones las funciones de selecci√≥n y reducci√≥n son funciones "orientadas a tarea", es decir, que se definen en funci√≥n de la tarea posterior.

Las funciones presentadas en el apartado ‚Äúfunciones de pesado‚Äù pueden emplearse tambi√©n como funciones de selecci√≥n de rasgos:
- Con funciones locales, se seleccionan los rasgos con mayor peso en cada documento y se genera con ellos un vocabulario reducido
- Con funciones globales, se seleccionan los rasgos con m√°s peso dentro de la colecci√≥n y se genera con ellos un vocabulario reducido.


### Selecci√≥n de caracter√≠sticas/rasgos en aprendizaje autom√°tico:

Objetivos:
- Reducir el sobreebtranamiento(overfitting): menos datos redundantes se traduce en eliminar ruido.
- Mejorar la precisi√≥n: con la eliminaci√≥n de ruido.
- Reducir el tiempo de entrenamiento: con la reducci√≥n de rasgos los algoritmos se entrenan m√°s r√°pido.

Algunas t√©cnicas:
- Selecci√≥n univariante: utilizar pruebas estad√≠sticas para seleccionar aquellas caracter√≠sticas que tienen la relaci√≥n m√°s fuerte con la variable de salida
- Importancia de las caracter√≠sticas: Se puede obtener la relevancia de cada caracter√≠stica del conjunto de datos utilizando la propiedad de importancia de la caracter√≠stica del modelo. La importancia de la caracter√≠stica da una puntuaci√≥n para cada caracter√≠stica de los datos, cuanto mayor sea la puntuaci√≥n, m√°s importante o relevante ser√° la caracter√≠stica para su variable de salida.
- Ganancia de informaci√≥n (Clasificaci√≥n): se usa para establecer la calidad de un determinado rasgo en una tarea de clasificaci√≥n.
- Informaci√≥n mutua (Clasificaci√≥n). Esta funci√≥n se ha empleado habitualmente en el contexto del modelado estad√≠stico del lenguaje, fundamentalmente para encontrar relaciones entre rasgos.


